<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>do stochastic gradient descent · Manopt.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><script src="../../../copy.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Manopt.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../about/">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../MeanAndMedian/">get Started: Optimize!</a></li><li><a class="tocitem" href="../GeodesicRegression/">Do Geodesic regression</a></li><li><a class="tocitem" href="../HowToRecord/">Record values</a></li><li class="is-active"><a class="tocitem" href>do stochastic gradient descent</a></li><li><a class="tocitem" href="../GradientOfSecondOrderDifference/">see the gradient of <span>$d_2$</span></a></li><li><a class="tocitem" href="../JacobiFields/">use Jacobi Fields</a></li><li><a class="tocitem" href="../../pluto/Benchmark/">speed up! using <code>gradF!</code></a></li><li><a class="tocitem" href="../../pluto/Bezier/">Use Bezier Curves</a></li><li><a class="tocitem" href="../../pluto/AutomaticDifferentiation/">AD in Manopt</a></li></ul></li><li><a class="tocitem" href="../../plans/">Plans</a></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../../solvers/">Introduction</a></li><li><a class="tocitem" href="../../solvers/alternating_gradient_descent/">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/ChambollePock/">Chambolle-Pock</a></li><li><a class="tocitem" href="../../solvers/conjugate_gradient_descent/">Conjugate gradient descent</a></li><li><a class="tocitem" href="../../solvers/cyclic_proximal_point/">Cyclic Proximal Point</a></li><li><a class="tocitem" href="../../solvers/DouglasRachford/">Douglas–Rachford</a></li><li><a class="tocitem" href="../../solvers/gradient_descent/">Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/NelderMead/">Nelder–Mead</a></li><li><a class="tocitem" href="../../solvers/particle_swarm/">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../../solvers/quasi_Newton/">Quasi-Newton</a></li><li><a class="tocitem" href="../../solvers/stochastic_gradient_descent/">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/subgradient/">Subgradient method</a></li><li><a class="tocitem" href="../../solvers/truncated_conjugate_gradient_descent/">Steihaug-Toint TCG Method</a></li><li><a class="tocitem" href="../../solvers/trust_regions/">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../../functions/">Introduction</a></li><li><a class="tocitem" href="../../functions/bezier/">Bézier curves</a></li><li><a class="tocitem" href="../../functions/costs/">Cost functions</a></li><li><a class="tocitem" href="../../functions/differentials/">Differentials</a></li><li><a class="tocitem" href="../../functions/adjointdifferentials/">Adjoint Differentials</a></li><li><a class="tocitem" href="../../functions/gradients/">Gradients</a></li><li><a class="tocitem" href="../../functions/Jacobi_fields/">Jacobi Fields</a></li><li><a class="tocitem" href="../../functions/proximal_maps/">Proximal Maps</a></li><li><a class="tocitem" href="../../functions/manifold/">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../../helpers/data/">Data</a></li><li><a class="tocitem" href="../../helpers/errorMeasures/">Error Measures</a></li><li><a class="tocitem" href="../../helpers/exports/">Exports</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../../notation/">Notation</a></li><li><a class="tocitem" href="../../list/">Function Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">How to...</a></li><li class="is-active"><a href>do stochastic gradient descent</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>do stochastic gradient descent</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/StochasticGradientDescent.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="SGDTutorial"><a class="docs-heading-anchor" href="#SGDTutorial">Stochastic Gradient Descent</a><a id="SGDTutorial-1"></a><a class="docs-heading-anchor-permalink" href="#SGDTutorial" title="Permalink"></a></h1><p>This tutorial illustrates how to use the <a href="../../solvers/stochastic_gradient_descent/#Manopt.stochastic_gradient_descent"><code>stochastic_gradient_descent</code></a> solver and different <a href="../../solvers/gradient_descent/#Manopt.DirectionUpdateRule"><code>DirectionUpdateRule</code></a>s in order to introduce the average or momentum variant, see <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent</a>.</p><p>Computationally we look at a very simple but large scale problem, the Riemannian Center of Mass or <a href="https://en.wikipedia.org/wiki/Fréchet_mean">Fréchet mean</a>: For given points <span>$p_i ∈\mathcal M$</span>, <span>$i=1,…,N$</span> this optimization problem reads</p><p class="math-container">\[\operatorname*{arg\,min}_{x∈\mathcal M} \frac{1}{2}\sum_{i=1}^{N}
  \operatorname{d}^2_{\mathcal M}(x,p_i),\]</p><p>which of course can be (and is) solved by a gradient descent, see the <a href="../MeanAndMedian/#Optimize">introductionary tutorial</a>. If <span>$N$</span> is very large it might be quite expensive to evaluate the complete gradient. A remedy is, to evaluate only one of the terms at a time and choose a random order for these.</p><p>We first initialize the manifold (see [])</p><pre><code class="language-julia hljs">using Manopt, Manifolds, Random, Colors</code></pre><p>and we define some colors from <a href="https://personal.sron.nl/~pault/">Paul Tol</a></p><pre><code class="language-julia hljs">black = RGBA{Float64}(colorant&quot;#000000&quot;)
TolVibrantOrange = RGBA{Float64}(colorant&quot;#EE7733&quot;) # Start
TolVibrantBlue = RGBA{Float64}(colorant&quot;#0077BB&quot;) # a path
TolVibrantTeal = RGBA{Float64}(colorant&quot;#009988&quot;) # points</code></pre><p>And optain a large data set</p><pre><code class="language-julia hljs">n = 5000
σ = π / 12
M = Sphere(2)
x = 1 / sqrt(2) * [1.0, 0.0, 1.0]
Random.seed!(42)
data = [exp(M, x, random_tangent(M, x, Val(:Gaussian), σ)) for i in 1:n]</code></pre><p>which looks like</p><pre><code class="language-julia hljs">asymptote_export_S2_signals(&quot;centerAndLargeData.asy&quot;;
    points = [ [x], data],
    colors=Dict(:points =&gt; [TolVibrantBlue, TolVibrantTeal]),
    dot_sizes = [2.5, 1.0], camera_position = (1.,.5,.5)
)
render_asymptote(&quot;centerAndLargeData.asy&quot;; render = 2)</code></pre><p><img src="../../assets/images/tutorials/centerAndLargeData.png" alt="The data of noisy versions of \$x\$"/></p><p>Note that due to the construction of the points as zero mean tangent vectors, the mean should be very close to our initial point <code>x</code>.</p><p>In order to use the stochastic gradient, we now need a function that returns the vector of gradients. There are two ways to define it in <code>Manopt.jl</code>: as one function, that returns a vector or a vector of funtions.</p><p>The first variant is of course easier to define, but the second is more efficient when only evaluating one of the gradients. For the mean we have as a gradient</p><p class="math-container">\[ gradF(x) = \sum_{i=1}^N \operatorname{grad}f_i(x) \quad \text{where} \operatorname{grad}f_i(x) = -\log_x p_i\]</p><p>Which we define as</p><pre><code class="language-julia hljs">F(M, x) = 1 / (2 * n) * sum(map(p -&gt; distance(M, x, p)^2, data))
gradF(M, x) = [grad_distance(M, p, x) for p in data]
gradf = [(M, x) -&gt; grad_distance(M, p, x) for p in data];</code></pre><p>The calls are only slightly different, but notice that accessing the 2nd gradient element requires evaluating all logs in the first function. So while you can use both <code>gradF</code> and <code>gradf</code> in the following call, the second one is faster:</p><pre><code class="language-julia hljs">@time x_opt1 = stochastic_gradient_descent(M, gradF, x);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">  0.266653 seconds (347.29 k allocations: 20.286 MiB, 13.87% gc time, 99.36% compilation time)</code></pre><p>versus</p><pre><code class="language-julia hljs">@time x_opt2 = stochastic_gradient_descent(M, gradf, x);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">  0.137367 seconds (175.71 k allocations: 9.714 MiB, 99.96% compilation time)</code></pre><p>This result is reasonably close. But we can improve it by using a <a href="../../solvers/gradient_descent/#Manopt.DirectionUpdateRule"><code>DirectionUpdateRule</code></a>, namely: On the one hand <a href="../../solvers/gradient_descent/#Manopt.MomentumGradient"><code>MomentumGradient</code></a>, which requires both the manifold and the initial value,    in order to keep track of the iterate and parallel transport the last direction to the current iterate.    you can also set a <code>vector_transport_method</code>, if <code>ParallelTransport()</code> is not    available on your manifold. Here we simply do</p><pre><code class="language-julia hljs">@time x_opt3 = stochastic_gradient_descent(
    M, gradf, x; direction=MomentumGradient(M, x, StochasticGradient(zero_vector(M, x)))
);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">  0.316865 seconds (722.62 k allocations: 53.731 MiB, 99.16% compilation time)</code></pre><p>And on the other hand the <a href="../../solvers/gradient_descent/#Manopt.AverageGradient"><code>AverageGradient</code></a> computes an average of the last <code>n</code> gradients, i.e.</p><pre><code class="language-julia hljs">@time x_opt4 = stochastic_gradient_descent(
    M, gradf, x; direction=AverageGradient(M, x, 10, StochasticGradient(zero_vector(M, x)))
);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">  0.415408 seconds (890.25 k allocations: 62.980 MiB, 9.55% gc time, 99.65% compilation time)</code></pre><p>note that the default <a href="../../solvers/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> is a fixed number of iterations.</p><p>Note that since you can apply both also in case of <a href="../../solvers/gradient_descent/#Manopt.gradient_descent"><code>gradient_descent</code></a>, i.e. to use <a href="../../solvers/gradient_descent/#Manopt.IdentityUpdateRule"><code>IdentityUpdateRule</code></a> and evaluate the classical gradient, both constructors have to know that internally the default evaluation of the Stochastic gradient (choosing one gradient <span>$\operatorname{grad}f_k$</span> at random) has to be specified.</p><p>For this small example you can of course also use a gradient descent with <a href="../../plans/#Manopt.ArmijoLinesearch"><code>ArmijoLinesearch</code></a>, but it will be a little slower usually</p><pre><code class="language-julia hljs">@time x_opt5 = gradient_descent(
    M, F, (M, x) -&gt; sum(gradF(M, x)), x; stepsize=ArmijoLinesearch()
);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">  1.819968 seconds (9.33 M allocations: 775.881 MiB, 12.53% gc time, 12.72% compilation time)</code></pre><p>but it is for sure faster than the variant above that evaluates the full gradient on every iteration, since stochastic gradient descent takes more iterations.</p><p>Note that all 5 of couse yield the same result</p><pre><code class="language-julia hljs">[distance(M, x, y) for y in [x_opt1, x_opt2, x_opt3, x_opt4, x_opt5]]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
 2.1073424255447017e-8
 2.1073424255447017e-8
 2.1073424255447017e-8
 2.1073424255447017e-8
 2.1073424255447017e-8</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../HowToRecord/">« Record values</a><a class="docs-footer-nextpage" href="../GradientOfSecondOrderDifference/">see the gradient of <span>$d_2$</span> »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.12 on <span class="colophon-date" title="Saturday 12 February 2022 12:03">Saturday 12 February 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
